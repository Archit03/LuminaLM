model:
  d_model: 512
  src_seq_len: 512
  batch_size: 32
  epochs: 5

tokenizer:
  save_path: "./tokens/Medical_tokenizer.json"
  load_path: "./tokens/Medical_tokenizer.json"
  vocab_size: 60000
  special_tokens: ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_samples: 100000
  shuffle_seed: 42
  num_workers: 4

training:
  learning_rate: 5.0e-5
  use_mixed_precision: true
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 1000
  weight_decay: 0.01
  dropout: 0.1
  validation_steps: 500
  gradient_noise_std: 0.0
  patience: 3
  scheduler: "linear"
  optimizer: "adamw"

logging:
  level: "INFO"
  save_dir: "logs"
  metrics_file: "metrics.json"
  log_frequency: 100

checkpointing:
  save_dir: "checkpoints"
  save_frequency: 1
  keep_best_n: 3
  metric_for_best: "val_loss"

visualization:
  plot_dir: "plots"
  sample_size: 1000
  embedding_dims: 3
  plot_frequency: 500

distributed:
  backend: "nccl"
  world_size: -1
  init_method: "env://"
  find_unused_parameters: false

pinecone:
  api_key: null
  environment: "us-east-1"
  index_name: "luminalm-embeddings"
  dimension: 512
  metric: "cosine"

device:
  type: "auto"
  precision: "float32"

model:
  d_model: 512
  src_seq_len: 512
  batch_size: 32
  learning_rate: 5e-5
  epochs: 5
  patience: 3

tokenizer:
  save_path: "embeddings/tokenizer.json"
  load_path: "embeddings/tokenizer.json"

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_samples: 100000

training:
  use_mixed_precision: true
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 1000
  weight_decay: 0.01
  dropout: 0.1
  validation_steps: 500  # Optional
  gradient_noise_std: 0.0  # Set >0 to add gradient noise

logging:
  level: INFO
  save_dir: "logs"
  metrics_file: "metrics.json"

checkpointing:
  save_dir: "checkpoints"
  save_frequency: 1
  keep_best_n: 3

visualization:
  plot_dir: "plots"
  sample_size: 1000
  embedding_dims: 3

distributed:
  backend: "nccl"
  world_size: -1  # auto-detect
  init_method: "env://"

pinecone:
  api_key: "e38016a0-15c6-4f83-aa30-9f3821a819fc"  # Use environment variable
  environment: "us-east-1"  # Use environment variable
  index_name: "luminalm-embeddings"
